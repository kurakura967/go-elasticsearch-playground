{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters \n",
    "\n",
    "XGBoost has a lot of parameters you can adjust to control the fitting of your model. The parameters can have a big impact on model performance and run time. See the [XGBoost docs](https://xgboost.readthedocs.io/en/latest/parameter.html) for a full list.\n",
    "\n",
    "Some of these parameters like `booster` are general and define which type of boosting is to be done. These general parameters impact which other parameters can be used down stream.\n",
    "\n",
    "For this lab and LTR in general we are focused on Regression trees. For trees we can select the `objective` function to control what is being optimized by the model. We can also control a variety of tree attributes.\n",
    "\n",
    "The goal of this lab is to show how hyparameters impact model perfomance and how we can can search for them effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "\n",
    "import ltr\n",
    "\n",
    "from matplotlib.pylab import rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(_____csv_of_features_____)\n",
    "feature_cols = [col for col in df if col.startswith('feature')] # https://stackoverflow.com/questions/27275236/pandas-best-way-to-select-all-columns-whose-names-start-with-x\n",
    "\n",
    "features = df[feature_cols]\n",
    "labels = df[['grade']]\n",
    "\n",
    "dmx = xgb.DMatrix(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test parameters\n",
    "\n",
    "We still need to define which parameters we want to search over, similar to the fields in the `netfix` notebook. We do this in a Python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example to get started\n",
    "test_params = {\n",
    " 'max_depth':[1,2,4,3],\n",
    " 'grow_policy': ['lossguide', 'depthwise']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search again\n",
    "\n",
    "This time we are grid searching in Python and we can take advantage of the `sklearn` library. XGBoost plays well with sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install sklearn into your notebook environment if it doesn't exist\n",
    "!pip3 install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = GridSearchCV(estimator = xgb.XGBRegressor(), param_grid = test_params)\n",
    "model.fit(features,labels)\n",
    "\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand the parameters you are checking\n",
    "# exploring the parameters [eta, gamma, lambda] are good bet\n",
    "\n",
    "test_params2 = {\n",
    " 'max_depth':[1,2,4,3],\n",
    " 'grow_policy': ['lossguide', 'depthwise'],\n",
    " ___.....____\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-run the GridSearch across your new expanded test_params2\n",
    "# Do the parameter estimates change?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combination explosion\n",
    "\n",
    "Some times you will find you have too many parameters for GridSearch to be economical, it would just take too long to complete.\n",
    "\n",
    "If that's the case there is a sibling function in `sklearn` called `RandomizedSearchCV`. This function works to identify good combinations, while allow you to set the maximum number of combinations to try. The search is not exhaustive like GridSearch is, so you might not find the best combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "RandomizedSearchCV??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 3, 'grow_policy': 'lossguide', 'gamma': 2, 'base_score': 0}\n"
     ]
    }
   ],
   "source": [
    "model = RandomizedSearchCV(estimator = xgb.XGBRegressor(), param_distributions=test_params2)\n",
    "model.fit(features,labels)\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 3, 'grow_policy': 'lossguide', 'gamma': 2, 'base_score': 0}\n"
     ]
    }
   ],
   "source": [
    "# Increase the default number of iterations and re-RandomizedSearch\n",
    "# Do you parameter estimates change?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decrease the number of iterations to 3\n",
    "# What happens in this case?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
